{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Content License Agreement**\n",
        "\n",
        "<font color='red'><b>**WARNING**</b></font> : 본 자료는 삼성 청년 SW아카데미의 컨텐츠 자산으로, 보안서약서에 의거하여 어떠한 사유로도 임의로 복사, 촬영, 녹음, 복제, 보관, 전송하거나 허가 받지 않은 저장매체를 이용한 보관, 제3자에게 누설, 공개 또는 사용하는 등의 무단 사용 및 불법 배포 시 법적 조치를 받을 수 있습니다."
      ],
      "metadata": {
        "id": "lOaRhtkLOBec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Objectives**\n",
        "\n",
        "1. 실습 개요\n",
        "    - LangChain의 개념을 이해하고 활용법을 학습.\n",
        "    - Upstage의 Solar LLM을 기반으로 LangChain의 ChatModel을 단계별로 구축.\n",
        "    - AI 기반 챗봇 개발 실습을 통해 실질적인 프로젝트 구현 능력 강화.\n",
        "\n",
        "\n",
        "2. 실습 진행 목적 및 배경\n",
        "    - 목적: LangChain과 Upstage Solar LLM을 활용해 AI 기반 언어 모델의 실질적인 응용 능력과 챗봇 개발 역량을 강화합니다.\n",
        "    - 배경: LLM은 다양한 산업에서 핵심 기술로 자리 잡고 있으며, LangChain은 이를 연결하고 확장하는 데 강력한 프레임워크로 활용됩니다. 이러한 Langchain 프레임워크를 학습하여 실무에 적용 가능한 역량을 강화합니다.\n",
        "\n",
        "\n",
        "3. 실습 수행으로 얻어갈 수 있는 역량\n",
        "    - AI 모델 API 활용 능력: Upstage Console에서 API Key를 발급받아 활용하고, Solar LLM을 호출하는 능력을 습득합니다.\n",
        "    - LangChain 이해 및 활용 능력: LangChain의 구조와 원리를 이해하고, 이를 바탕으로 LLM Chain을 설계하고 구현할 수 있습니다.\n",
        "    - 실무형 챗봇 개발 역량: API와 LangChain을 결합하여 AI 기반 챗봇을 개발하며, 실무에서 요구되는 기술 역량을 기릅니다.\n",
        "\n",
        "4. 실습 핵심 내용\n",
        "    - 환경 설정: 실습에 필요한 환경을 구성합니다.\n",
        "        - 필수 라이브러리 설치\n",
        "        - API Key 발급 및 활용법 익히기\n",
        "    - Solar LLM 호출: Upstage의 Solar LLM을 호출하여 기본적인 작동 원리를 확인합니다.\n",
        "    - LangChain 개념 이해: LangChain의 핵심 구조와 사용법을 학습합니다.\n",
        "    - LLM Chain 구현: LangChain을 사용하여 간단한 LLM Chain을 설계하고 실행합니다."
      ],
      "metadata": {
        "id": "_PpqBnqAsnxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prerequisites**"
      ],
      "metadata": {
        "id": "kFNQOdQjKwdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "langchain == 0.3.12\n",
        "langchain-chroma == 0.1.4\n",
        "langchain-core == 0.3.25\n",
        "langchain-openai == 0.2.12\n",
        "langchain-text-splitters == 0.3.3\n",
        "langchain-upstage == 0.4.0\n",
        "getpass4 == 0.0.14.1\n",
        "openai == 1.57.4\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Sxu1MdnsKvt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 라이브러리 소개\n",
        "  - `openai` : AI 모델과 상호작용할 수 있는 라이브러리\n",
        "  - `langchain` : 다양한 언어 모델 및 자연어 처리 기능들을 하나의 프레임워크에서 사용할 수 있도록 도와주는 라이브러리\n",
        "  -`langchain-upstage` : Upstage의 다양한 기술을 Langchain에서 활용할 수 있도록 지원하는 라이브러리\n",
        "  - `langchain-chroma` : Chroma 데이터베이스와 LangChain을 연동하여 데이터 검색과 처리를 더 효율적으로 수행할 수 있게 도와주는 라이브러리\n",
        "  - `getpass4` : 터미널에서 비밀번호나 중요한 정보를 보이지 않게 입력받을 수 있도록 하는 라이브러리\n",
        "\n"
      ],
      "metadata": {
        "id": "ZtRFJsB_K0N3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 약 1분 소요\n",
        "!pip install -qU openai langchain langchain-upstage langchain-chroma getpass4"
      ],
      "metadata": {
        "id": "aQ7vuYeUK1OW",
        "outputId": "cdbe194c-cee4-40ba-c725-3b3bba7051d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for clipboard (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "transformers 4.47.1 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.19.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise Overview**\n",
        "\n",
        "## **실습 목차**\n",
        "- **Session 1. 환경 설정**: 실습에 필요한 환경을 미리 세팅해둡니다. <br>\n",
        "  - 1-1. Hello Solar!\n",
        "    - 1 발급받은 API Key 실습 파일에 적용하기\n",
        "    - 2 API Key를 사용하여 Upstage Solar Chat Model 활용해보기\n",
        "\n",
        "- **Session 2. LLM Chain 이해하기** : LangChain을 사용하여 LLM Chain을 구현하는 방법을 배웁니다. <br>\n",
        "    - 2-1 LLM Chain\n",
        "    - 2-2 What is Langchain?"
      ],
      "metadata": {
        "id": "6-0k6D6jK2NN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. 환경 설정**\n",
        "\n",
        "## **1-1. HELLO SOLAR!**\n",
        "\n",
        "Upstage API Key를 발급받고 이를 통해 Solar-Mini-Chat Model을 호출해봅니다."
      ],
      "metadata": {
        "id": "k27eBG7ysN2A"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIO-9AGTrKVp"
      },
      "source": [
        "#### 1-1-1 UPSTAGE Credit 및 API Key 발급 받기\n",
        "\n",
        "1. 회원 가입 진행\n",
        "  1. <a href = \"https://console.upstage.ai/\">업스테이지 콘솔</a> 에 방문합니다.\n",
        "  2. 계정이 없다면, 구글 계정을 통해 회원가입을 진행합니다\n",
        "  3.  계정에 로그인 합니다.\n",
        "\n",
        "2. API Key 발급\n",
        "  1. <a href = \"https://console.upstage.ai/api-keys\">업스테이지 콘솔 - API Keys</a>페이지를 클릭합니다.\n",
        "  2. Create New key를 누르고, 발급받은 API key를 복사합니다.\n",
        "  3. 하단 셀을 실행한 후, 복사한 API Key를 넣습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRvAhpCtrKVq",
        "outputId": "1f37bb3c-0dc8-4910-f9dc-c436922edb9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Upstage API key: ··········\n",
            "API key has been set successfully.\n"
          ]
        }
      ],
      "source": [
        "# @title set API key\n",
        "import os\n",
        "import getpass\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Get the Upstage API key using getpass\n",
        "if \"UPSTAGE_API_KEY\" not in os.environ or not os.environ[\"UPSTAGE_API_KEY\"]:\n",
        "    os.environ[\"UPSTAGE_API_KEY\"] = getpass.getpass(\"Enter your Upstage API key: \")\n",
        "\n",
        "print(\"API key has been set successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9hcvEkQrKVq"
      },
      "source": [
        "### 1-1-2 Solar Chat Model 사용해보기"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 💻 코드 설명\n",
        "\n",
        "1. API Key와 기본 URL을 설정\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1tE21AG588xjXseoxt9iDxduEl8DmmuC0\" alt=\"Solar Model comparison\" width=\"800\"/>\n",
        "\n",
        "\n",
        "2. 채팅 요청을 생성\n",
        "   - 활용하고자 하는 **Chat 모델** 지정:  `\"solar-pro\"` or `\"solar-mini\"`\n",
        "   - Prompt(프롬프트) 지정\n",
        "      - Prompt란?\n",
        "        - LLM에게 Input 과 함께 제공하는 지시사항 혹은 예제들\n",
        "      - `System Prompt`\n",
        "        -  LLM에게 특정 역할 또는 작업(task)를 부여하여 응답을 생성할 수 있도록 설정할 수 있습니다.\n",
        "        -  대화가 포멀한지 캐주얼한 지, 친절한 어조 혹은 격식 있는 어조 등 대화 스타일을 설정할 수 있습니다.\n",
        "      - `User Prompt`\n",
        "        - 사용자가 LLM에게 보낼 쿼리/질문을 의미합니다.\n",
        "\n",
        "3. 모델의 응답 처리:\n",
        "   - `print()` :  전체 응답을 출력\n",
        "   - `response.choices[0].message.content` : 답변만 출력"
      ],
      "metadata": {
        "id": "PyR2xTkXzDKr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmN7hAXSrKVq",
        "outputId": "cd91a941-5582-4575-dab2-e386fa082aed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='65188799-0648-4341-8773-14fa827686f4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='한국의 수도는 서울입니다.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1735406509, model='solar-pro-241126', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=8, prompt_tokens=52, total_tokens=60, completion_tokens_details=None, prompt_tokens_details=None))\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from pprint import pprint\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=os.environ[\"UPSTAGE_API_KEY\"], base_url=\"https://api.upstage.ai/v1/solar\"\n",
        ")\n",
        "\n",
        "chat_result = client.chat.completions.create(\n",
        "    model= \"solar-pro\",# 사용할 모델 입력\n",
        "    messages=[\n",
        "        # system prompt\n",
        "        {'role' : 'system', 'content' : '답변은 항상 존댓말로 친절하게 해주세요. 정확한 답변을 하고, 답변할 내용이 없거나 모르면 답변하지 말아주세요.'}\n",
        "        ,\n",
        "        # user prompt\n",
        "        {'role' : 'user', 'content' : '한국의 수도는 무엇인가요?'}\n",
        "    ],\n",
        ")\n",
        "\n",
        "pprint(chat_result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_result.choices[0].message.content)"
      ],
      "metadata": {
        "id": "GI4XNR_3xVkD",
        "outputId": "5fb7da07-14b0-4683-cf3a-bb8338fe6c05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한국의 수도는 서울입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Few Shot Prompting**\n",
        "  - 모델이 더 자연스럽고 맥락에 맞는 응답을 생성할 수 있도록 몇 가지 예제를 제공하는 기법입니다. 이를 통해 모델이 주어진 입력에 어떻게 응답해야 하는지 학습할 수 있습니다.\n",
        "  - 위와 같이 아무 예시를 들지 않는 경우를 **Zero-Shot Prompting**이라고 하며, 아래는 예시를 여러개 제시한 **Few-Shot Prompting**의 예시입니다.\n",
        "  - `assistant` 라는 role을 지정하여 예시를 넣어줍니다"
      ],
      "metadata": {
        "id": "Wns5NsfX65pN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# few shot prompts: examples or history\n",
        "chat_result = client.chat.completions.create(\n",
        "    model= \"solar-pro\",# 사용할 모델 입력\n",
        "    messages=[\n",
        "        # system prompt\n",
        "        {'role' : 'system', 'content' : '답변은 항상 존댓말로 친절하게 해주세요. 정확한 답변을 하고, 답변할 내용이 없거나 모르면 답변하지 말아주세요.'}\n",
        "        ,\n",
        "        # user prompt\n",
        "        {'role' : 'user', 'content' : '한국의 수도는 무엇인가요?'},      # human query\n",
        "        {'role' : 'assistant', 'content' : '한국의 수도는 서울입니다.'},  # LLM answer\n",
        "        {'role' : 'user', 'content' : '일본의 수도는 무엇인가요?'},      # human query\n",
        "        {'role' : 'assistant', 'content' : '일본의 수도는 도쿄입니다.'},  # LLM answer\n",
        "        {'role' : 'user', 'content' : '중국의 수도는 무엇인가요?'},      # human query\n",
        "        {'role' : 'assistant', 'content' : '중국의 수도는 베이징입니다.'},  # LLM answer\n",
        "\n",
        "        # user query\n",
        "        {'role' : 'user', 'content' : '베트남의 수도는 무엇인가요?'},      # human query\n",
        "    ],\n",
        ")\n",
        "\n",
        "# pprint(chat_result)\n",
        "print(chat_result.choices[0].message.content)"
      ],
      "metadata": {
        "id": "_GRL2iKd6QH4",
        "outputId": "2570d18a-f7db-4a44-e946-b132fd994807",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "베트남의 수도는 하노이입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb86u_82rKVr"
      },
      "source": [
        "# **2.  LLM Chain과 Langchain 이해하기**\n",
        "\n",
        "- 2-1 LLM Chain\n",
        "  - LLM Chain이 무엇인지 이해합니다.\n",
        "- 2-2 What is Langchain?\n",
        "  - LLM Chain을 제공하는 Langchain 라이브러리에 대한 설명을 제공합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Recap.) LLM의 구성요소\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1UUy0ulWxBe5BapXufz6oT5cR9_As8Qta\" alt=\"Definition of DP\" width=\"600\" />\n",
        "\n",
        "1. `User Input` : 사용자가 LLM에게 보낼 질문/쿼리/입력\n",
        "2. `Prompt`\n",
        "  - Task Description : LLM에게 특정 역할/task를 부여함. (e.g. 질의응답, 요약, 번역 등)\n",
        "  - Output Indicator : 대화 스타일, 어조 등을 부여하는 등 output 형식들을 지정\n",
        "  - Example Inputs : Few-Shot Prompting, LLM이 맥락을 더 파악할 수 있도록 예제를 넣어줌\n",
        "3. `LLM Model` : user input을 바탕으로 output을 생성할 대규모 텍스트 데이터로 사전학습된 AI 모델\n",
        "4. `Output` : LLM이 생성해내는 Text 답변/응답\n"
      ],
      "metadata": {
        "id": "8q-Qfmer0MEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **위 구성요소들 (User Input, Prompt, LLM Model)을 손쉽게 연결해서 쉽게 output 답변을 생성할 수 있는 도구가 \"LLM Chain\"**\n",
        "\n"
      ],
      "metadata": {
        "id": "2aunQ_xW3JtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2-1 LLM Chain**"
      ],
      "metadata": {
        "id": "11psffuAkd9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### 📘 LLM Chain이란?\n",
        "\n",
        " LLM Chain(체인)이란, **여러 구성 요소들을 결합**하여 **LLM의 출력을 생성하는 프로세스**를 의미합니다. 이는 입력 프롬프트를 모델에 전달하고, 모델의 출력을 받아 원하는 형식으로 처리할 수 있도록 구성된 파이프라인입니다. 각 단계별로 어떻게 Chain을 구성하는 지 알아봅시다.\n",
        "\n",
        "#### LLM Chain 구성 요소\n",
        "- **LLM(Large Language Model)**  : 모델 정의\n",
        "- **Prompt(프롬프트)** : LLM에게 쿼리(질문)과 함께 입력하는 예제와 지시사항  \n",
        "- Output Parser(출력 파서, *optional*) : 출력 결과의 형태를 지정할 수 있는 도구\n",
        "\n",
        "\n",
        "### langchain_upstage\n",
        "\n",
        "- langchain을 사용할 수 있는 여러 Provider들(e.g. upstage, huggingface, openai, google 등)이 직접 제공하는 Langchain의 통합 모듈.\n",
        "\n",
        "> https://python.langchain.com/docs/integrations/providers/upstage/"
      ],
      "metadata": {
        "id": "0AmtPjSGhmNi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 💻 코드 설명\n",
        "\n",
        "1. 선호하는 LLM 정의:\n",
        "   - `langchain_upstage`에서 `ChatUpstage` 클래스를 가져옵니다.\n",
        "   - `ChatUpstage` 인스턴스를 생성하고 변수 `llm`에 할당합니다.\n",
        "\n",
        "2. 입력 프롬프트 정의:\n",
        "   - `langchain_core.prompts`에서 `ChatPromptTemplate` 클래스를 가져옵니다.\n",
        "   - `from_messages()` 메서드를 사용해 `ChatPromptTemplate` 인스턴스를 생성합니다.\n",
        "   -  system prompt, few-shot prompting을 위한 예제들, user input 등을 포함한 메시지 리스트를 제공합니다.\n",
        "\n",
        "3. 출력 파서 정의 :\n",
        "   - `langchain_core.output_parsers`에서 `StrOutputParser` 클래스를 가져옵니다.\n",
        "   - 더 자세한 Parser에 대한 내용은 [Langchain Guide](https://python.langchain.com/docs/concepts/output_parsers/) 참고 바랍니다.\n",
        "\n",
        "4. 체인 정의 :\n",
        "   - `rag_with_history_prompt`, `llm`, `StrOutputParser()`를 파이프(`|`) 연산자를 사용하여 결합해 체인을 만듭니다.\n",
        "\n",
        "4. 체인 호출:\n",
        "   - `chain` 객체의 `invoke()` 메서드를 호출하고 빈 딕셔너리(`{}`)를 입력값으로 전달합니다.\n",
        "   - 체인에서 얻은 응답을 출력합니다."
      ],
      "metadata": {
        "id": "xHh5Cuj_LBwd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2-2 What is Langchain?**"
      ],
      "metadata": {
        "id": "ui1xXvOjkeC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://media.licdn.com/dms/image/D4D12AQGQQFHNeQJRgQ/article-cover_image-shrink_720_1280/0/1711873462713?e=2147483647&v=beta&t=u5ls9p4LHatE_PxtiNIm23lIFGMaAjp-XHdV7TwwDxE\" alt=\"Langchain\" width=\"300\" />\n",
        "\n",
        "- LLM Chain을 제공해주는 라이브러리!\n",
        "\n",
        "- LangChain은 대규모 언어 모델(LLM)을 활용해 AI Application(e.g.챗봇, QA 시스템 등)을 쉽게 개발할 수 있도록 도와주는 프레임워크입니다. 이는 누구나 LLM을 이용해 쉽고 빠르게 애플리케이션을 만들고 배포할 수 있도록 지원합니다. 프로그래밍 경험이 없어도 LangChain의 다양한 도구와 템플릿을 통해 필요한 AI 애플리케이션을 만들 수 있습니다. 복잡한 개발 과정을 간소화하여 AI 애플리케이션 개발을 더 쉽게 접근할 수 있게 해줍니다.\n",
        "\n",
        "\n",
        "#### Langchain의 구성요소\n",
        "1. [Langchain Library](https://python.langchain.com/v0.2/docs/introduction/) : Lanchain의 다양한 기능을 사용할 수 있게 구현해둔 패키지\n",
        "   - **langchain-core**: LangChain의 가장 기본 문법\n",
        "   - **Integrated Package** : 다른 AI 도구와 LangChain을 쉽게 연결할 수 있는 패키지 (e.g. [langchain-upstage](https://python.langchain.com/docs/integrations/providers/upstage/), [langchain_chroma](https://python.langchain.com/docs/integrations/providers/chroma/))\n",
        "   - **langchain**: 애플리케이션 구성에 필요한 체인, 에이전트, 정보 검색 전략 등을 제공하여 애플리케이션의 '두뇌' 역할을 합니다.\n",
        "\n",
        "2. [Langchain Templates](https://templates.langchain.com/): 다양한 작업에 맞춘 템플릿으로, 개발자들이 애플리케이션을 더 빠르게 설정하고 실행할 수 있도록 돕습니다.\n",
        "3. [LangServe](https://python.langchain.com/docs/langserve/): LangChain으로 만든 애플리케이션을 REST API로 쉽게 배포할 수 있게 해주는 도구입니다.\n",
        "4. [LangSmith](https://docs.smith.langchain.com/): 개발자가 애플리케이션을 디버그하고 테스트하며 모니터링할 수 있도록 도와주는 플랫폼입니다.\n",
        "5. [LangGraph](https://www.langchain.com/langgraph): LLM이 가지는 여러 상태들을 관리하여 Agent를 구조화하여 구현할 수 있는 프레임워크입니다.\n",
        "\n",
        "더 자세한 Langchain에 대한 설명은 <a href = \"https://python.langchain.com/docs/introduction/\">Langchain Documentation</a> 을 확인하세요!"
      ],
      "metadata": {
        "id": "BrYTbL214uDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM Chain 구성하는 법.\n",
        "# 1. llm 정의, 2. prompt 정의, 3. chain 정의, 4. chain 호출\n",
        "\n",
        "from langchain_upstage import ChatUpstage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 1. llm 정의\n",
        "llm = ChatUpstage()\n",
        "\n",
        "# 2. prompt 정의\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        # system prompt\n",
        "        ('human', '답변은 항상 존댓말로 친절하게 해주세요. 정확한 답변을 하고, 답변할 내용이 없거나 모르면 답변하지 말아주세요.')\n",
        "        ,\n",
        "        # user prompt\n",
        "        ('human', '한국의 수도는 무엇인가요?'),      # human query\n",
        "        ('ai', '한국의 수도는 서울입니다.'),        # LLM answer\n",
        "        ('human', '일본의 수도는 무엇인가요?'),      # human query\n",
        "        ('ai', '일본의 수도는 도쿄입니다.'),        # LLM answer\n",
        "        ('human', '중국의 수도는 무엇인가요?'),      # human query\n",
        "        ('ai', '중국의 수도는 베이징입니다.'),        # LLM answer\n",
        "\n",
        "        # user query\n",
        "        ('human', '베트남의 수도는 무엇인가요?'),      # human query\n",
        "    ],\n",
        ")\n",
        "\n",
        "# 3. chain 정의\n",
        "output = StrOutputParser()\n",
        "chain = prompt | llm | output\n",
        "\n",
        "# 4. chain 호출\n",
        "result = chain.invoke({})\n",
        "result"
      ],
      "metadata": {
        "id": "C1aPIN8E595P",
        "outputId": "dcf772f5-6152-4e36-cbd3-9bde58d5357c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'베트남의 수도는 하노이입니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf_F36k2rKV5"
      },
      "source": [
        "# 🚀 실습 마무리\n",
        "\n",
        " 이번 실습을 통해 Upstage Solar 모델을 활용하여 ChatBot을 단계별로 구현하는 방법에 대해 알아보았습니다. 다음 실습에서는 RAG를 붙여서 구현하는 방법에 대해 알아보도록 하겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fwbcvHgNmeez"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}