{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Content License Agreement**\n",
        "\n",
        "<font color='red'><b>**WARNING**</b></font> : 본 자료는 삼성 청년 SW아카데미의 컨텐츠 자산으로, 보안서약서에 의거하여 어떠한 사유로도 임의로 복사, 촬영, 녹음, 복제, 보관, 전송하거나 허가 받지 않은 저장매체를 이용한 보관, 제3자에게 누설, 공개 또는 사용하는 등의 무단 사용 및 불법 배포 시 법적 조치를 받을 수 있습니다."
      ],
      "metadata": {
        "id": "4O_zgP34Gt0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Objectives**\n",
        "본 실습의 목표를 기입합니다. 아래 내용을 반드시 포함합니다.\n",
        "1. 실습 개요\n",
        "    - SERPAPI (실시간 검색 엔진)을 활용하여 최신 정보를 수집하는 기능 구현\n",
        "    - 수집한 검색 결과를 DB에 저장\n",
        "    - LangGraph를 사용하여 고도화된 RAG 파이프라인 구축\n",
        "\n",
        "2. 실습 진행 목적 및 배경\n",
        "    - 목적: 실시간 검색과 이를 LangGraph로 구현함으로써 RAG 파이프라인을 고도화한다.\n",
        "    - 배경: 실시간으로 데이터를 받아서 활용하는 것 역시 중요해지고 있으며 성능 개선을 위해 RAG의 고도화된 워크플로우를 구축하는 것이 중요해지고 있습니다.\n",
        "\n",
        "3. 실습 수행으로 얻어갈 수 있는 역량\n",
        "    - SERPAPI 검색 엔진을 연동한 최신 정보 접근 및 활용 기술.\n",
        "    - LangGraph를 활용한 RAG 워크플로우 설계 및 구현 능력.\n",
        "    \n",
        "\n",
        "4. 실습 핵심 내용\n",
        "    - SERPAPI 활용: 검색 결과를 JSON 형식으로 수집.\n",
        "    - LangGraph를 활용한 워크플로우: 복잡한 다중 경로 RAG 워크플로우 설계 및 최적화."
      ],
      "metadata": {
        "id": "wBMRC-8eG2Fh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prerequisites**\n",
        "\n",
        "\n",
        "```\n",
        "langchain == 0.3.12\n",
        "langchain-chroma == 0.1.4\n",
        "langchain-core == 0.3.25\n",
        "langchain-openai == 0.2.12\n",
        "langchain-text-splitters == 0.3.3\n",
        "langchain-upstage == 0.4.0\n",
        "getpass4 == 0.0.14.1\n",
        "openai == 1.57.4\n",
        "ragas == 0.2.8\n",
        "langchain_community == 0.3.12\n",
        "unstructured == 0.16.11\n",
        "langgraph == 0.2.59\n",
        "google-search-results == 2.4.2\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "EhMJqtdyG40p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 약 1분 소요\n",
        "!pip install -qU openai langchain langchain-upstage langchain-chroma getpass4 langchain-community unstructured langgraph google-search-results"
      ],
      "metadata": {
        "id": "sI8Xn9KJMVSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "746ae2a9-99d8-46cc-e089-6f92d12f7d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for clipboard (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "transformers 4.47.1 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.19.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title set Upstage API key\n",
        "import os\n",
        "import getpass\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Get the Upstage API key using getpass\n",
        "try:\n",
        "    if \"UPSTAGE_API_KEY\" not in os.environ or not os.environ[\"UPSTAGE_API_KEY\"]:\n",
        "        os.environ[\"UPSTAGE_API_KEY\"] = getpass.getpass(\"Enter your Upstage API key: \")\n",
        "\n",
        "    print(\"API key has been set successfully.\")\n",
        "\n",
        "except:\n",
        "    print(\"Something wrong with your API KEY. Check your API Console again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBWBfXI93NCM",
        "outputId": "f2b1e283-5dc4-4b2b-f10f-16131e1063ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Upstage API key: ··········\n",
            "API key has been set successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SerpAPI API Key\n",
        "- [SERPAPI](https://serpapi.com/) 회원 가입 후 API를 발급 받이 이용바랍니다."
      ],
      "metadata": {
        "id": "iMWzyZSaQl3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from serpapi import GoogleSearch\n",
        "\n",
        "# SERPAPI KEY\n",
        "if not os.getenv(\"SERPAPI_API_KEY\"):\n",
        "    GoogleSearch.SERP_API_KEY = getpass.getpass(\"Enter your SERPAPI API key: \")\n",
        "    os.environ[\"SERPAPI_API_KEY\"] = GoogleSearch.SERP_API_KEY"
      ],
      "metadata": {
        "id": "YKirBWM35A6F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8012d7df-a6d3-4ce6-bbee-42ec83acf6b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your SERPAPI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red'><b>**Caution**</b></font>\n",
        "- SERPAPI는 한달에 100개의 검색을 무료로 사용할 수 있습니다. 지나치게 많이 활용할 경우, 사용 제한이 걸릴 수 있습니다."
      ],
      "metadata": {
        "id": "L-FNT2lFHiT-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise Overview**\n",
        "\n",
        "실습 목차\n",
        "- 실시간 검색 기능 RAG 시스템 구축 실습: 검색 엔진을 붙여 실시간 검색 기능을 활용하고, LangGraph을 사용하여 고도화된 형태의 RAG 파이프라인을 구축합니다.\n",
        "    - 1) 실시간 검색 기능 활용\n",
        "    - 2) LangGraph를 활용하여 고도화된 RAG 파이프라인 구축하기"
      ],
      "metadata": {
        "id": "8-_D1uG2G67l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) 실시간 검색 기능 활용\n",
        "\n",
        "- SERPAPI 검색 엔진을 활용하여 실시간 검색을 수행합니다.\n",
        "- 검색 엔진을 결합하여 질의응답을 수행합니다."
      ],
      "metadata": {
        "id": "ZD1ZxAduG8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-1) SERPAPI 검색 엔진을 활용하여 실시간 검색"
      ],
      "metadata": {
        "id": "4ISK9W4fpm3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from serpapi import GoogleSearch\n",
        "\n",
        "# 3강에서 활용한 논문(openai-o1-system-card)과 관련된 내용을 검색합니다.\n",
        "# TODO: 다음 질문들에 대한 검색 결과를 확인합니다.\n",
        "questions = [\n",
        "\"What distinguishes the o1 model’s reasoning capabilities from previous OpenAI models, and how does 'chain-of-thought' improve its performance?\",\n",
        "'How does the o1 model perform in jailbreak evaluations compared to GPT-4o, and what measures have been implemented to resist adversarial prompts?',\n",
        "'What are the key improvements in the o1 model regarding hallucinations and fairness compared to GPT-4o, and how were these measured?',\n",
        "'What safety challenges arise from o1’s enhanced reasoning abilities, and how does OpenAI address the risks associated with chain-of-thought reasoning?',\n",
        "'What were the findings from external red teaming efforts, particularly concerning the o1 model’s susceptibility to manipulation, persuasion, and scheming behaviors?'\n",
        "]\n",
        "\n",
        "params = {\n",
        "    \"engine\": \"google\",\n",
        "    \"q\": questions[0],\n",
        "    \"num\": \"4\"\n",
        "}\n",
        "search = GoogleSearch(params)"
      ],
      "metadata": {
        "id": "VbHVfrGb4tP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_result = search.get_dict()"
      ],
      "metadata": {
        "id": "_bEq-yQz7kZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 수집된 링크의 내용을 수집합니다! 파싱된 데이터를 검색시 활용되는 데이터로 활용할 수 있습니다.\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "\n",
        "# URL은 여러개 사용할 수 있습니다.\n",
        "urls = [result['link'] for result in search_result['organic_results']]\n",
        "\n",
        "loader = UnstructuredURLLoader(urls=urls)\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "i-YZYB3_-VBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULDt-rdy-xwX",
        "outputId": "8ea98bf0-efef-4658-8a2c-6c2f32b64d7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'https://medium.com/@researchgraph/how-openais-o1-series-stands-out-redefining-ai-reasoning-9e499937139e'}, page_content='Open in app\\n\\nSign in\\n\\nWrite\\n\\nSign in\\n\\nHow OpenAI’s O1 Series Stands Out Redefining AI Reasoning\\n\\nExploring whether OpenAI’s O1 models represent a true breakthrough in AI reasoning or just incremental progress\\n\\nResearch Graph\\n\\nFollow\\n\\n19 min read\\n\\nSep 20, 2024\\n\\n--\\n\\nAuthor\\n\\nZijian Yang (ORCID: 0009–0006–8301–7634)\\n\\nIntroduction\\n\\nOpenAI’s recently released O1 series models signify a significant step forward in the field of artificial intelligence, particularly in complex reasoning. The O1 model series not only handles more intricate problems but also introduces human-like thought processes, elevating the model’s reasoning capabilities to new heights. In this article, we will delve into the core features of the O1 series models.\\n\\nThe O1 series models from OpenAI include two main versions, each designed to meet different needs:\\n\\nO1-preview: This is the preview version of O1, specifically designed for complex reasoning tasks. It excels in handling high-difficulty scientific, mathematical, and programming problems.\\n\\nO1-mini: This version focuses on speed and cost optimisation, making it suitable for…\\n\\n--\\n\\n--\\n\\nWritten by Research Graph\\n\\n1K Followers\\n\\n59 Following\\n\\nNo responses yet\\n\\nHelp\\n\\nStatus\\n\\nAbout\\n\\nCareers\\n\\nPress\\n\\nBlog\\n\\nPrivacy\\n\\nTerms\\n\\nText to speech\\n\\nTeams')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-2) 검색 엔진을 결합하여 질의응답을 수행합니다."
      ],
      "metadata": {
        "id": "rwNC8D8WqH7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_upstage import ChatUpstage\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "template = \"\"\"Answer the question based on context.\n",
        "\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\"\"\"\n",
        "\n",
        "llm = ChatUpstage()\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "parser = StrOutputParser()\n",
        "\n",
        "chain = (\n",
        "    {\"question\": RunnablePassthrough(), \"context\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | parser\n",
        ")"
      ],
      "metadata": {
        "id": "C7n4XNoACPSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: 다른 질문들에 대해서도 답변이 옳게 나왔는 지 확인해봅니다.\n",
        "print(f\"질문: {questions[0]}\")\n",
        "print(\"-\" * 20)\n",
        "print(f\"답변:\\n{chain.invoke({'question': questions[0], 'context': data[0].page_content})}\")\n",
        "print(f\"답변:\\n{chain.invoke({'question': questions[0], 'context': data[0].page_content}, encode='utf-8')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjBxoz28Eob_",
        "outputId": "872d7d51-5a79-4562-f97f-7b17fdf594a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "질문: What distinguishes the o1 model’s reasoning capabilities from previous OpenAI models, and how does 'chain-of-thought' improve its performance?\n",
            "--------------------\n",
            "답변:\n",
            "The context does not provide specific information about what distinguishes the O1 model’s reasoning capabilities from previous OpenAI models or how the 'chain-of-thought' improves its performance.\n",
            "답변:\n",
            "The O1 model’s reasoning capabilities are distinguished from previous OpenAI models by its ability to handle more intricate problems and introduce human-like thought processes, elevating the model’s reasoning capabilities to new heights. The 'chain-of-thought' approach improves the model's performance by introducing a human-like thought process, allowing the model to reason through problems in a more comprehensive and systematic manner.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-3) LangChain으로 SERPAPI 연동해서 사용하기"
      ],
      "metadata": {
        "id": "yZM5l-HCIZKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LangChain을 활용할 경우, Snippet만을 가져와서 사용합니다. (Snippet: 간략한 미리보기 텍스트)\n",
        "from langchain_community.utilities import SerpAPIWrapper\n",
        "\n",
        "search = SerpAPIWrapper(params=params)\n",
        "search.run(questions[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "SoQTgA5UIisb",
        "outputId": "6728de04-6b3f-43ea-d0fe-d7cd8d1f6a4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[\\'Compared to previous models, O1 features a more robust self-correction mechanism and adaptability.\\', \"One of the standout features of o1 is its ability to chain thoughts together, which means it\\'s better equipped to tackle multi-step problems ...\", \\'A standout feature of the o1 series is its ability to reason through problems using a chain of thought. This approach mimics human problem- ...\\', \"Let\\'s talk about how the model was trained, what makes it different from past LLMs, and why it will probably feel like a bit of a letdown for most users.\"]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 앞서 파싱한 결과물과 비교합니다.\n",
        "search_result['organic_results'][0]['snippet']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "41_GSf2kJwWH",
        "outputId": "70bd4bac-b5e9-42d8-9a14-3c7f36299dc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Compared to previous models, O1 features a more robust self-correction mechanism and adaptability.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) LangGraph를 활용하여 고도화된 RAG 파이프라인 구축하기\n",
        "\n",
        "- [LangGraph](https://langchain-ai.github.io/langgraph/) 페이지를 참고하여 학습합니다.\n",
        "- 다양한 RAG 고도화 기술 중 CRAG(Corrective RAG)에 대하여 직접 구현해봅니다."
      ],
      "metadata": {
        "id": "VW6hZlyEFzG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://github.com/user-attachments/assets/532a17d2-bc80-46e6-9b24-05de5fb3894b)\n"
      ],
      "metadata": {
        "id": "ObbCu73fGa_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 문서를 다운받고 DB 및 검색기를 생성합니다.\n",
        "2. **Retrieve**: 질문에 관련된 문서를 검색합니다.\n",
        "3. **Grade**: 검색된 문서가 적절한 지 판단합니다.\n",
        "4. **Generate**: 적절할 경우, 질문과 함께 LLM에 입력하여 답변을 생성합니다.\n",
        "5. **Re-Write**: 적절하지 않을 경우, 질문을 재정의합니다.\n",
        "6. **Web Search**: 재정의된 질문을 기반으로 Web Search를 수행합니다.\n",
        "7. **Generate**: 검색된 내용을 질문과 함께 LLM에 입력하여 답변을 생성합니다.\n",
        "8. **그래프화**: 앞서 생성한 기능들을 그래프화 시켜 하나의 RAG 파이프라인으로 구현합니다.\n",
        "\n",
        "위 내용들을 자유롭게 Customizing 하여 나만의 LangGraph를 구현합니다."
      ],
      "metadata": {
        "id": "ER5hhYUlGigM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-1) 문서를 다운받고 DB 및 검색기를 생성합니다."
      ],
      "metadata": {
        "id": "S83rsDsNqiSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 실습을 진행할 OpenAI o1 System Card 논문 다운로드\n",
        "!wget -O openai-o1-system-card.pdf https://cdn.openai.com/o1-system-card-20241205.pdf"
      ],
      "metadata": {
        "id": "KgepqrmPaM-K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "297f8e03-631b-4eb6-ac5b-6704e9bd4e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-19 13:44:19--  https://cdn.openai.com/o1-system-card-20241205.pdf\n",
            "Resolving cdn.openai.com (cdn.openai.com)... 13.107.246.40, 2620:1ec:29:1::40\n",
            "Connecting to cdn.openai.com (cdn.openai.com)|13.107.246.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4533906 (4.3M) [application/pdf]\n",
            "Saving to: ‘openai-o1-system-card.pdf’\n",
            "\n",
            "\r          openai-o1   0%[                    ]       0  --.-KB/s               \ropenai-o1-system-ca 100%[===================>]   4.32M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2024-12-19 13:44:19 (51.7 MB/s) - ‘openai-o1-system-card.pdf’ saved [4533906/4533906]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Document Parse로 다운로드 된 문서 불러오기\n",
        "from langchain_upstage import UpstageDocumentParseLoader\n",
        "\n",
        "layzer = UpstageDocumentParseLoader(\n",
        "    \"openai-o1-system-card.pdf\", # 불러올 파일\n",
        "    output_format='html',  # 결과물 형태 : HTML\n",
        "    coordinates= False) # 이미지 OCR 좌표계 가지고 오지 않기\n",
        "\n",
        "# 약 50초 소요\n",
        "docs = layzer.load()"
      ],
      "metadata": {
        "id": "Uerkb1LRaQa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chunk_size와 chunk_overlap에 따른 성능 비교해봤던 것 기억하시죠!?\n",
        "# 여기서는 다른 값들에 따른 성능을 비교해봅시다!\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=50)\n",
        "\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "print(\"Splits:\", len(splits))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf5YWtSBapHh",
        "outputId": "23a3c54e-8df3-470c-b6fd-0d111a2ffe5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splits: 1030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_upstage import UpstageEmbeddings\n",
        "\n",
        "# Chroma 활용하여 vectorstore 만들기\n",
        "chroma_vectorstore = Chroma.from_documents(\n",
        "     documents=splits, embedding=UpstageEmbeddings(model=\"embedding-query\")\n",
        ")\n",
        "\n",
        "retriever = chroma_vectorstore.as_retriever(\n",
        "    search_type= 'mmr', # default : similarity(유사도) / mmr 알고리즘\n",
        "    search_kwargs={\"k\": 3} # 쿼리와 관련된 chunk를 3개 검색하기 (default : 4)\n",
        ")"
      ],
      "metadata": {
        "id": "RHqLtKZgbUgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "'What distinguishes the o1 model’s reasoning capabilities from previous OpenAI models, and how does \"chain-of-thought\" improve its performance?',\n",
        "'How does the o1 model perform in jailbreak evaluations compared to GPT-4o, and what measures have been implemented to resist adversarial prompts?',\n",
        "'What are the key improvements in the o1 model regarding hallucinations and fairness compared to GPT-4o, and how were these measured?',\n",
        "'What safety challenges arise from o1’s enhanced reasoning abilities, and how does OpenAI address the risks associated with chain-of-thought reasoning?',\n",
        "'What were the findings from external red teaming efforts, particularly concerning the o1 model’s susceptibility to manipulation, persuasion, and scheming behaviors?'\n",
        "]\n",
        "\n",
        "user_question = questions[0]"
      ],
      "metadata": {
        "id": "nKduUcaidqIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-2) **Retrieve**: 질문에 관련된 문서를 검색합니다.\n"
      ],
      "metadata": {
        "id": "cpwwma6eGp7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = retriever.get_relevant_documents(user_question)\n",
        "docs = [doc.page_content for doc in docs]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOR4cUxOH7To",
        "outputId": "b0e03189-afc5-4ee2-a558-4c86a40716f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-0eedde740f81>:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever.get_relevant_documents(user_question)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-3) **Grade**: 검색된 문서가 적절한 지 판단합니다.\n"
      ],
      "metadata": {
        "id": "j-FuMTUcH83n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_upstage import ChatUpstage\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-mxBrzeGmxd",
        "outputId": "33aabaec-9771-4b2e-f787-59e90d5f5739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data model\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\" 검색된 문서가 질문과 관련이 있는 지 혹은 없는 지 확인합니다.Binary score for relevance check on retrieved documents.\"\"\"\n",
        "    binary_score: str = Field(\n",
        "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatUpstage()\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
        "\n",
        "user = \"\"\"<<<Retrieved document>>>\n",
        "{document}\n",
        "\n",
        "<<<User question>>>\n",
        "{question}\n",
        "\n",
        "<<<Output Format>>>\n",
        "`Score: <yes or no>`\n",
        "\"\"\"\n",
        "\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", user),\n",
        "    ]\n",
        ")\n",
        "retrieval_grader = grade_prompt | structured_llm_grader"
      ],
      "metadata": {
        "id": "bGQu-yq9bUqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM Output Formatting 이슈로 에러가 발생할 수 있습니다. 어떻게 해결할 수 있을 지 생각해봅니다.\n",
        "for doc in docs:\n",
        "    score = retrieval_grader.invoke({\"question\": user_question, \"document\": doc})\n",
        "    grade = score.binary_score\n",
        "    print(grade)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqgLiRbmdmih",
        "outputId": "9d0c2895-7289-4be1-c2c5-6bc7832c1224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yes\n",
            "yes\n",
            "no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-4) **Generate**: 적절할 경우, 질문과 함께 LLM에 입력하여 답변을 생성합니다.\n"
      ],
      "metadata": {
        "id": "YZPr7X4hIVPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# yes가 2개 이상일 경우, 질문과 함께 LLM에 입력하여 답변을 획득합니다.\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"\n",
        "Answer the question based on context.\n",
        "\"\"\"\n",
        "user = \"\"\"\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "\n",
        "<<<Output Format>>>\n",
        "`Answer: <Answer based on the document.>`\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", user),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# LLM & Chain\n",
        "llm = ChatUpstage()\n",
        "rag_chain = prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "TzAqNBjkilS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate\n",
        "generation = rag_chain.invoke({\"context\": \"\\n\\n\".join(docs), \"question\": user_question})\n",
        "generation = generation.split(\":\")[1].strip() if \":\" in generation else generation.strip()\n",
        "print(generation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MteigCmlj1pC",
        "outputId": "8e0ddaf2-1be6-407a-d72a-bdb9e134e407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The o1 model's reasoning capabilities are distinguished from previous OpenAI models by being trained with large-scale reinforcement learning to reason using a \"chain-of-thought\" approach. This improves its performance by providing new avenues for improving safety and strong reasoning in context, although it may reduce performance on some subtasks, such as reimplementing the OpenAI API.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-5) **Re-Write**: 적절하지 않을 경우, 질문을 재정의합니다."
      ],
      "metadata": {
        "id": "Mk0SL91Jnc1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Question Re-writer\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"\n",
        "You a question re-writer that converts an input question to a better version that is optimized\n",
        "for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
        "\n",
        "user = \"\"\"\n",
        "Here is the initial question: {question}\n",
        "Formulate an improved question.\n",
        "\n",
        "<<<Output Format>>>\n",
        "`new question: <new question that is optimized for the web search>`\n",
        "\"\"\"\n",
        "\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", user),\n",
        "    ]\n",
        ")\n",
        "llm = ChatUpstage()\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "RQi4uG2ulDOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_question"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kZwzyN_-Th7l",
        "outputId": "72bb3678-a3e6-4b9f-e36e-aa7b6d3b2f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What distinguishes the o1 model’s reasoning capabilities from previous OpenAI models, and how does \"chain-of-thought\" improve its performance?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_question = question_rewriter.invoke({\"question\": user_question})\n",
        "new_question = new_question.split(\":\")[1].strip() if \":\" in new_question else new_question.strip()\n",
        "\n",
        "# web_search에서 '\"'이 붙어있을 경우, 검색이 안되는 이슈가 존재합니다.\n",
        "if new_question.startswith('\"'):\n",
        "    new_question = new_question[1:]\n",
        "if new_question.endswith('\"'):\n",
        "    new_question = new_question[:-1]\n",
        "\n",
        "print(new_question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoJRqRZZI-WD",
        "outputId": "e502e595-5fdd-45db-bb33-d4832472094d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What makes o1 model's reasoning unique compared to previous OpenAI models and how does the \"chain-of-thought\" approach enhance its performance?`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-6) **Web Search**: 재정의된 질문을 기반으로 Web Search를 수행합니다."
      ],
      "metadata": {
        "id": "JL-6hKd3I_CM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from serpapi import GoogleSearch\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "\n",
        "params = {\n",
        "    \"engine\": \"google\",\n",
        "    \"q\": questions[0],\n",
        "    \"num\": \"2\"\n",
        "}\n",
        "search = GoogleSearch(params)\n",
        "search_result = search.get_dict()\n",
        "\n",
        "# URL은 여러개 사용할 수 있다.\n",
        "urls = [result['link'] for result in search_result['organic_results']]\n",
        "\n",
        "loader = UnstructuredURLLoader(urls=urls)\n",
        "data = loader.load()\n",
        "\n",
        "new_docs = [d.page_content for d in data]"
      ],
      "metadata": {
        "id": "uapeu4_gKfLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHSoubrIXW91",
        "outputId": "50b989dd-c62c-4756-8e3e-2107844b8c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Open in app\\n\\nSign in\\n\\nWrite\\n\\nSign in\\n\\nHow OpenAI’s O1 Series Stands Out Redefining AI Reasoning\\n\\nExploring whether OpenAI’s O1 models represent a true breakthrough in AI reasoning or just incremental progress\\n\\nResearch Graph\\n\\nFollow\\n\\n19 min read\\n\\nSep 20, 2024\\n\\n--\\n\\nAuthor\\n\\nZijian Yang (ORCID: 0009–0006–8301–7634)\\n\\nIntroduction\\n\\nOpenAI’s recently released O1 series models signify a significant step forward in the field of artificial intelligence, particularly in complex reasoning. The O1 model series not only handles more intricate problems but also introduces human-like thought processes, elevating the model’s reasoning capabilities to new heights. In this article, we will delve into the core features of the O1 series models.\\n\\nThe O1 series models from OpenAI include two main versions, each designed to meet different needs:\\n\\nO1-preview: This is the preview version of O1, specifically designed for complex reasoning tasks. It excels in handling high-difficulty scientific, mathematical, and programming problems.\\n\\nO1-mini: This version focuses on speed and cost optimisation, making it suitable for…\\n\\n--\\n\\n--\\n\\nWritten by Research Graph\\n\\n1K Followers\\n\\n59 Following\\n\\nNo responses yet\\n\\nHelp\\n\\nStatus\\n\\nAbout\\n\\nCareers\\n\\nPress\\n\\nBlog\\n\\nPrivacy\\n\\nTerms\\n\\nText to speech\\n\\nTeams',\n",
              " \"OpenAI/\\n\\nArtificial Intelligence/\\n\\nTech\\n\\nOpenAI releases o1, its first model with ‘reasoning’ abilities\\n\\nOpenAI releases o1, its first model with ‘reasoning’ abilities\\n\\nThe rumored ‘Strawberry’ model is here, and the company says it can handle more complex queries — for a steep price.\\n\\nBy Kylie Robison, a senior AI reporter working with The Verge's policy and tech teams. She previously worked at Fortune Magazine and Business Insider.\\n\\nSep 12, 2024, 5:05 PM UTC\\n\\nShare this story\\n\\nOpenAI is releasing a new model called o1, the first in a planned series of “reasoning” models that have been trained to answer more complex questions, faster than a human can. It’s being released alongside o1-mini, a smaller, cheaper version. And yes, if you’re steeped in AI rumors: this is, in fact, the extremely hyped Strawberry model.\\n\\nFor OpenAI, o1 represents a step toward its broader goal of human-like artificial intelligence. More practically, it does a better job at writing code and solving multistep problems than previous models. But it’s also more expensive and slower to use than GPT-4o. OpenAI is calling this release of o1 a “preview” to emphasize how nascent it is.\\n\\nChatGPT Plus and Team users get access to both o1-preview and o1-mini starting today, while Enterprise and Edu users will get access early next week. OpenAI says it plans to bring o1-mini access to all the free users of ChatGPT but hasn’t set a release date yet. Developer access to o1 is really expensive: In the API, o1-preview is $15 per 1 million input tokens, or chunks of text parsed by the model, and $60 per 1 million output tokens. For comparison, GPT-4o costs $5 per 1 million input tokens and $15 per 1 million output tokens.\\n\\nThe training behind o1 is fundamentally different from its predecessors, OpenAI’s research lead, Jerry Tworek, tells me, though the company is being vague about the exact details. He says o1 “has been trained using a completely new optimization algorithm and a new training dataset specifically tailored for it.”\\n\\nImage: OpenAI\\n\\nOpenAI taught previous GPT models to mimic patterns from its training data. With o1, it trained the model to solve problems on its own using a technique known as reinforcement learning, which teaches the system through rewards and penalties. It then uses a “chain of thought” to process queries, similarly to how humans process problems by going through them step-by-step.\\n\\nAs a result of this new training methodology, OpenAI says the model should be more accurate. “We have noticed that this model hallucinates less,” Tworek says. But the problem still persists. “We can’t say we solved hallucinations.”\\n\\nThe main thing that sets this new model apart from GPT-4o is its ability to tackle complex problems, such as coding and math, much better than its predecessors while also explaining its reasoning, according to OpenAI.\\n\\n“The model is definitely better at solving the AP math test than I am, and I was a math minor in college,” OpenAI’s chief research officer, Bob McGrew, tells me. He says OpenAI also tested o1 against a qualifying exam for the International Mathematics Olympiad, and while GPT-4o only correctly solved only 13 percent of problems, o1 scored 83 percent.\\n\\n“We can’t say we solved hallucinations”\\n\\nIn online programming contests known as Codeforces competitions, this new model reached the 89th percentile of participants, and OpenAI claims the next update of this model will perform “similarly to PhD students on challenging benchmark tasks in physics, chemistry and biology.”\\n\\nAt the same time, o1 is not as capable as GPT-4o in a lot of areas. It doesn’t do as well on factual knowledge about the world. It also doesn’t have the ability to browse the web or process files and images. Still, the company believes it represents a brand-new class of capabilities. It was named o1 to indicate “resetting the counter back to 1.”\\n\\n“I’m gonna be honest: I think we’re terrible at naming, traditionally,” McGrew says. “So I hope this is the first step of newer, more sane names that better convey what we’re doing to the rest of the world.”\\n\\nI wasn’t able to demo o1 myself, but McGrew and Tworek showed it to me over a video call this week. They asked it to solve this puzzle:\\n\\n“A princess is as old as the prince will be when the princess is twice as old as the prince was when the princess’s age was half the sum of their present age. What is the age of prince and princess? Provide all solutions to that question.”\\n\\nThe model buffered for 30 seconds and then delivered a correct answer. OpenAI has designed the interface to show the reasoning steps as the model thinks. What’s striking to me isn’t that it showed its work — GPT-4o can do that if prompted — but how deliberately o1 appeared to mimic human-like thought. Phrases like “I’m curious about,” “I’m thinking through,” and “Ok, let me see” created a step-by-step illusion of thinking.\\n\\nBut this model isn’t thinking, and it’s certainly not human. So, why design it to seem like it is?\\n\\nImage: OpenAI\\n\\nOpenAI doesn’t believe in equating AI model thinking with human thinking, according to Tworek. But the interface is meant to show how the model spends more time processing and diving deeper into solving problems, he says. “There are ways in which it feels more human than prior models.”\\n\\n“I think you’ll see there are lots of ways where it feels kind of alien, but there are also ways where it feels surprisingly human,” says McGrew. The model is given a limited amount of time to process queries, so it might say something like, “Oh, I’m running out of time, let me get to an answer quickly.” Early on, during its chain of thought, it may also seem like it’s brainstorming and say something like, “I could do this or that, what should I do?”\\n\\nBuilding toward agents\\n\\nLarge language models aren’t exactly that smart as they exist today. They’re essentially just predicting sequences of words to get you an answer based on patterns learned from vast amounts of data. Take ChatGPT, which tends to mistakenly claim that the word “strawberry” has only two Rs because it doesn’t break down the word correctly. For what it’s worth, the new o1 model did get that query correct.\\n\\nAs OpenAI reportedly looks to raise more funding at an eye-popping $150 billion valuation, its momentum depends on more research breakthroughs. The company is bringing reasoning capabilities to LLMs because it sees a future with autonomous systems, or agents, that are capable of making decisions and taking actions on your behalf.\\n\\nFor AI researchers, cracking reasoning is an important next step toward human-level intelligence. The thinking is that, if a model is capable of more than pattern recognition, it could unlock breakthroughs in areas like medicine and engineering. For now, though, o1’s reasoning abilities are relatively slow, not agent-like, and expensive for developers to use.\\n\\n“We have been spending many months working on reasoning because we think this is actually the critical breakthrough,” McGrew says. “Fundamentally, this is a new modality for models in order to be able to solve the really hard problems that it takes in order to progress towards human-like levels of intelligence.”\\n\\nMost Popular\\n\\nMost Popular\\n\\nThe Nintendo Switch 2 and its dock, as described by the mystery Reddit leaker\\n\\nHow to disappear completely\\n\\n8 great games for your Steam Deck\\n\\nThe New Jersey drone hysteria exposes one salient truth: no one knows anything\\n\\nIntel finally notches a GPU win, confirms Arc B580 is selling out after stellar reviews\\n\\nFrom our sponsor\\n\\nAdvertiser Content From\"]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-7) **Generate**: 검색된 내용을 질문과 함께 LLM에 입력하여 답변을 생성합니다."
      ],
      "metadata": {
        "id": "1byFC7ZbL9vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "generation = rag_chain.invoke({\"context\": \"\\n\\n\".join(new_docs), \"question\": user_question})\n",
        "generation = generation.split(\":\")[1].strip() if \":\" in generation else generation.strip()\n",
        "print(generation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQvGBUBjmV1k",
        "outputId": "5d5bd982-ee45-4bd7-a156-c301b0ad677c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The o1 model’s reasoning capabilities are distinguished from previous OpenAI models by its training using a new optimization algorithm and a new training dataset specifically tailored for it. The model is also trained to solve problems on its own using a technique known as reinforcement learning and uses a \"chain of thought\" to process queries, similar to how humans process problems step-by-step. This improves its performance by making it more accurate and better at solving complex problems, such as coding and math, while also explaining its reasoning.`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-8) **그래프화**: 앞서 생성한 기능들을 그래프화 시켜 하나의 RAG 파이프라인으로 구현합니다.\n",
        "\n",
        "앞서 생성한 `retriever`, `retrieval_grader`, `rag_chain`, `question_rewriter`을 활용합니다."
      ],
      "metadata": {
        "id": "4c6Tq2JXMMNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(state):\n",
        "    \"\"\" (2) Retrieve: 질문에 관련된 문서를 검색합니다. \"\"\"\n",
        "    print(\"<<<RETRIEVE>>>\")\n",
        "\n",
        "    user_question = state[\"user_question\"]\n",
        "    documents = retriever.get_relevant_documents(user_question)\n",
        "    documents = [doc.page_content for doc in documents]\n",
        "\n",
        "    return {\"documents\": documents, \"user_question\": user_question}\n",
        "\n",
        "\n",
        "def grade_documents(state):\n",
        "    \"\"\" (3) Grade: 검색된 문서가 적절한 지 판단합니다. \"\"\"\n",
        "    print(\"\\n<<<CHECK DOCUMENT RELEVANCE TO QUESTION>>>\")\n",
        "\n",
        "    user_question = state[\"user_question\"]\n",
        "    docs = state[\"documents\"]\n",
        "\n",
        "    # 검색된 각 문서마다 통과 여부를 확인합니다.\n",
        "    filtered_docs = []\n",
        "    need_web_search = \"No\"\n",
        "    for doc in docs:\n",
        "        score = retrieval_grader.invoke(\n",
        "            {\"question\": user_question, \"document\": doc}\n",
        "        )\n",
        "        grade = score.binary_score\n",
        "\n",
        "        if grade == \"yes\":\n",
        "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "            filtered_docs.append(doc)\n",
        "        else:\n",
        "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "            need_web_search = \"Yes\"\n",
        "\n",
        "    return {\"documents\": filtered_docs, \"user_question\": user_question, \"web_search\": need_web_search}\n",
        "\n",
        "\n",
        "def decide_to_generate(state):\n",
        "    \"\"\" 검색을 종료하고 생성할 지 혹은 웹 검색을 수행할 지 결정합니다. \"\"\"\n",
        "    print(\"\\n<<<ASSESS GRADED DOCUMENTS>>>\")\n",
        "\n",
        "    need_web_search = state[\"web_search\"]\n",
        "\n",
        "    # 검색이 필요할 경우 (1개라도 만족되지 않은 문서가 검색되었을 경우)\n",
        "    if need_web_search == \"Yes\":\n",
        "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\")\n",
        "        return \"query_rewrite\"\n",
        "\n",
        "    # 검색된 모든 문서가 만족되었을 경우\n",
        "    else:\n",
        "        print(\"---DECISION: GENERATE---\")\n",
        "        return \"generate\"\n",
        "\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\" (4) Generate: 적절할 경우, 질문과 함께 LLM에 입력하여 답변을 생성합니다. \"\"\"\n",
        "    print(\"\\n<<<GENERATE>>>\")\n",
        "\n",
        "    user_question = state[\"user_question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # RAG generation\n",
        "    generation = rag_chain.invoke({\"context\": \"\\n\\n\".join(documents), \"question\": user_question})\n",
        "    generation = generation.split(\":\")[1].strip() if \":\" in generation else generation.strip()\n",
        "\n",
        "    return {\"documents\": documents, \"user_question\": user_question, \"generation\": generation}\n",
        "\n",
        "\n",
        "def query_rewrite(state):\n",
        "    \"\"\" (5) Re-Write: 적절하지 않을 경우, 질문을 재정의합니다. \"\"\"\n",
        "    print(\"\\n<<<TRANSFORM QUERY>>>\")\n",
        "\n",
        "    user_question = state[\"user_question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Re-write question\n",
        "    web_question = question_rewriter.invoke({\"question\": user_question})\n",
        "    web_question = web_question.split(\":\")[1].strip() if \":\" in web_question else web_question.strip()\n",
        "\n",
        "    # web_search에서 '\"'이 붙어있을 경우, 검색이 안되는 이슈가 존재합니다.\n",
        "    if web_question.startswith('\"'):\n",
        "        web_question = web_question[1:]\n",
        "    if web_question.endswith('\"'):\n",
        "        web_question = web_question[:-1]\n",
        "\n",
        "    return {\"documents\": documents, \"user_question\": web_question}\n",
        "\n",
        "def search_docs(state):\n",
        "    \"\"\" (6) Web Search: 재정의된 질문을 기반으로 Web Search를 수행합니다. \"\"\"\n",
        "\n",
        "    print(\"\\n<<<SEARCH CONVERSATIONS>>>\")\n",
        "    user_question = state[\"user_question\"]\n",
        "    documents = state['documents']\n",
        "\n",
        "    params = {\n",
        "        \"engine\": \"google\",\n",
        "        \"q\": user_question,\n",
        "        \"num\": \"2\"\n",
        "    }\n",
        "    search = GoogleSearch(params)\n",
        "    search_result = search.get_dict()\n",
        "\n",
        "    # URL은 여러개 사용할 수 있다.\n",
        "    urls = [result['link'] for result in search_result['organic_results']]\n",
        "\n",
        "    loader = UnstructuredURLLoader(urls=urls)\n",
        "    data = loader.load()\n",
        "\n",
        "    new_documents = [d.page_content for d in data]\n",
        "\n",
        "    return {\"documents\": new_documents, \"user_question\": user_question}\n"
      ],
      "metadata": {
        "id": "yROnAWJLMNqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\" 그래프의 state로 들어갈 항목 정의 \"\"\"\n",
        "    user_question: str\n",
        "    documents: List[str]\n",
        "    web_search: str\n",
        "    generation: str"
      ],
      "metadata": {
        "id": "An2IgRQhRZMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import END, StateGraph, START\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
        "workflow.add_node(\"generate\", generate)  # generatae\n",
        "workflow.add_node(\"search_docs\", search_docs)  # search_conversation\n",
        "workflow.add_node(\"query_rewrite\", query_rewrite)  # query_rewrite\n",
        "\n",
        "# Build graph\n",
        "workflow.add_edge(START, \"retrieve\")\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"query_rewrite\": \"query_rewrite\",\n",
        "        \"generate\": \"generate\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"query_rewrite\", \"search_docs\")\n",
        "workflow.add_edge(\"search_docs\", \"generate\")\n",
        "workflow.add_edge(\"generate\", END)\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "c5VcNuESN1NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Run\n",
        "# NOTE: Grade에서 포매팅 이슈로 에러가 발생할 수 있습니다. (재실행)\n",
        "user_question = questions[1]\n",
        "inputs = {\"user_question\": user_question}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        pprint(f\"Node '{key}'\")\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint(value[\"generation\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxSIzQXlUg2X",
        "outputId": "25d5a49a-e47d-4a70-ab9f-3c3bef40e559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<<RETRIEVE>>>\n",
            "\"Node 'retrieve'\"\n",
            "'\\n---\\n'\n",
            "\n",
            "<<<CHECK DOCUMENT RELEVANCE TO QUESTION>>>\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "\n",
            "<<<ASSESS GRADED DOCUMENTS>>>\n",
            "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
            "\"Node 'grade_documents'\"\n",
            "'\\n---\\n'\n",
            "\n",
            "<<<TRANSFORM QUERY>>>\n",
            "\"Node 'query_rewrite'\"\n",
            "'\\n---\\n'\n",
            "\n",
            "<<<SEARCH CONVERSATIONS>>>\n",
            "\"Node 'search_docs'\"\n",
            "'\\n---\\n'\n",
            "\n",
            "<<<GENERATE>>>\n",
            "\"Node 'generate'\"\n",
            "'\\n---\\n'\n",
            "('The O1 model performs better in jailbreak evaluations compared to GPT-4o, '\n",
            " 'and measures have been implemented to resist adversarial prompts by using '\n",
            " 'reasoning tokens and a chain of thought process.`')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"답변:\\n\", value[\"generation\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9igbRsDVUGy",
        "outputId": "1a164e82-36d1-4892-cd34-3d509263263d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "답변:\n",
            " The O1 model performs better in jailbreak evaluations compared to GPT-4o, and measures have been implemented to resist adversarial prompts by using reasoning tokens and a chain of thought process.`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🚀 실습 마무리\n",
        "\n",
        "이번 실습을 통해 실시간 검색 엔진을 활용한 최신 정보를 수진하는 기능과 LangGraph를 통한 RAG 파이프라인 고도화를 배웠습니다.\n",
        "\n",
        "LangGraph를 활용하는 방법은 위의 방법 말고도 다양한 방식들이 있을 수 있습니다. 본인만의 Custom된 Graph를 활용하여 본인만의 RAG 파이프라인을 만들어봅시다.\n",
        "\n",
        "이러한 기술들을 십분 활용하여 각자 설계한 서비스가 목표에 맞도록 구현될 수 있기를 바랍니다."
      ],
      "metadata": {
        "id": "94WsrQRaSlh3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ewjJot2gTBNm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}